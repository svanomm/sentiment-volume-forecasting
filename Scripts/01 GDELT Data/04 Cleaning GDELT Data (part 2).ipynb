{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b20e9184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pickle, warnings, datetime, pytz\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer, MinMaxScaler\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ac759a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/processed/gdelt_pca.pkl', 'rb') as f:\n",
    "    pca_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7d465aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which times to keep based on the stock data\n",
    "with open(r\"../../Data/Processed/stock_data_simple.pkl\", 'rb') as f:\n",
    "    stock_data = pickle.load(f)\n",
    "\n",
    "stock_data['date'] = stock_data.index.date\n",
    "days = list(stock_data['date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9e8dc4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../../data/processed/gdelt_intermediate_cleaned.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a0bf4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.to_pandas()\n",
    "\n",
    "# Drop duplicate rows\n",
    "df.drop_duplicates(subset=['GKGRECORDID'], inplace=True)\n",
    "\n",
    "df.index = df['GKGRECORDID']\n",
    "df.drop(columns=['GKGRECORDID'], inplace=True)\n",
    "df.drop(columns=['Positive Score','Negative Score','Activity Reference Density','Self/Group Reference Density'], inplace=True)\n",
    "df.drop(columns=[i for i in df.columns if 'SCOREDVALUE' in i], inplace=True)\n",
    "df.drop(columns=[i for i in df.columns if 'WORDCOUNT' in i], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9faf026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, pca_data, on='GKGRECORDID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "611aa96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7c13e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create topic-specific metrics columns\n",
    "df['Article Count'] = 1\n",
    "\n",
    "df['general'] = 1\n",
    "\n",
    "topics  = ['general','Alaska Airlines','American Airlines','Delta Air Lines','JetBlue','Southwest Airlines','United Airlines','Allegiant Air']\n",
    "metrics = ['Tone','Polarity','Word Count',\n",
    "           'PCA_GKG1_0','PCA_GKG1_1','PCA_GKG1_2','PCA_GKG1_3','PCA_GKG1_4',\n",
    "           'PCA_Scored_0','PCA_Scored_1','PCA_Scored_2','PCA_Scored_3','PCA_Scored_4',\n",
    "           'PCA_Word_0','PCA_Word_1','PCA_Word_2','PCA_Word_3','PCA_Word_4',\n",
    "           'Article Count']\n",
    "\n",
    "for topic in topics:\n",
    "    for metric in metrics:\n",
    "        df[f'{metric}_{topic}'] = df[metric] * df[topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bc829965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grouped dataframe, grouped by datetime, that creates a sum for each metric\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "grouped_df = df.groupby('datetime').agg(\n",
    "    {f'{metric}_{topic}': ['sum'] for topic in topics for metric in metrics}\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex columns\n",
    "grouped_df.columns = ['_'.join(col).strip().replace('_sum','') for col in grouped_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ba12881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in all missing times\n",
    "start = datetime.datetime(2018, 1, 1, 0, 15, 0)\n",
    "end   = datetime.datetime(2025, 5, 31, 23, 45, 0)\n",
    "dates = pd.date_range(start=start, end=end, freq='15min')\n",
    "grouped_df = grouped_df.reindex(dates).reset_index()\n",
    "grouped_df = grouped_df.fillna(0)\n",
    "\n",
    "grouped_df['datetime'] = grouped_df['index']\n",
    "grouped_df.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2f012e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from UTC to EST, accounting for daylight saving time\n",
    "grouped_df['datetime'] = pd.to_datetime(grouped_df['datetime'], utc=True)\n",
    "grouped_df['datetime_EST'] = grouped_df['datetime'].dt.tz_convert('America/New_York')\n",
    "grouped_df['time'] = grouped_df['datetime_EST'].dt.time\n",
    "grouped_df['date'] = grouped_df['datetime_EST'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "97d572ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Handling after-hours articles #####\n",
    "\n",
    "# Join with stock data\n",
    "grouped_df['stock_time'] = np.where(grouped_df['date'].isin(days), grouped_df['datetime_EST'], pd.NaT)\n",
    "# Limit times after 15:45 and before 9:15\n",
    "grouped_df['stock_time'] = np.where(grouped_df['time'] > datetime.time(15,45,0), pd.NaT, grouped_df['stock_time'])\n",
    "grouped_df['stock_time'] = np.where(grouped_df['time'] < datetime.time(9,30,0) , pd.NaT, grouped_df['stock_time'])\n",
    "# format the stock_time column\n",
    "grouped_df['stock_time'] = pd.to_datetime(grouped_df['stock_time'])\n",
    "grouped_df = grouped_df.sort_values(by='datetime')\n",
    "# Backfill the stock_time2 column\n",
    "grouped_df['stock_time'] = grouped_df['stock_time'].ffill().bfill()\n",
    "# Remove the timezone information \n",
    "grouped_df['stock_time'] = grouped_df['stock_time'].dt.tz_localize(None)\n",
    "#grouped_df.drop(columns=['datetime', 'datetime_EST', 'time', 'date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7fa81d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The choice of window here ultimately affects how much after-hours time should be counted towards market open\n",
    "# For example, a 4-period window would mean that articles from 8:15 to 9:15 are counted towards the 9:30 period\n",
    "windows = [4, 8, 16, 48, 96]\n",
    "\n",
    "for window in windows:\n",
    "    for topic in topics:\n",
    "        for metric in metrics:\n",
    "            grouped_df[f'{metric}_{topic}_{window}'] = grouped_df[f'{metric}_{topic}'].rolling(window, min_periods=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "401eb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we keep the last of each stock time to remove duplicate values.\n",
    "grouped_df = grouped_df.drop_duplicates(subset=['stock_time'], keep='last')\n",
    "grouped_df.index = grouped_df['stock_time']\n",
    "grouped_df.drop(columns=['stock_time'], inplace=True)\n",
    "grouped_df.sort_index(inplace=True)\n",
    "grouped_df.drop(columns=['datetime','datetime_EST','time','date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "67182c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 1-period difference for each metric\n",
    "for topic in topics:\n",
    "    for metric in metrics:\n",
    "        grouped_df[f'Change_{metric}_{topic}'] = grouped_df[f'{metric}_{topic}'].diff()\n",
    "\n",
    "        for window in windows:\n",
    "            grouped_df[f'Change_{metric}_{topic}_{window}'] = grouped_df[f'{metric}_{topic}_{window}'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7042e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to pickle object\n",
    "with open(r\"../../Data/Processed/GDELT_Clean_202506191330.pkl\", 'wb') as f:\n",
    "    pickle.dump(grouped_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cef9293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the vars\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for window in windows:\n",
    "    for topic in topics:\n",
    "        for metric in metrics:\n",
    "            grouped_df[f'{metric}_{topic}_{window}'] = scaler.fit_transform(grouped_df[[f'{metric}_{topic}_{window}']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svo-directed-practicum (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
